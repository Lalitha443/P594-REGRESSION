# -*- coding: utf-8 -*-
"""Regression_Project (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ruHLIMsutEudr28uxL9BDNkaJ3h144ZG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('solarpowergeneration.csv')

df.head()

df.columns = df.columns.str.strip().str.lower().str.replace('[^a-z0-9_]', '_', regex=True)

"""# 1.Data Cleaning

## 1.1 Checking inconsitencies in the dataset
"""

df.info() # to check inconsitencies in the dataset

"""## 1.2 Checking null values"""

df.isnull().sum() # to check null values

# Replace null value with median in the average-wind-speed column
df.fillna(df['average_wind_speed__period_'].median(),inplace=True)

df.isnull().sum().sum() # null values has been treated well

"""# 1.3 Check duplicates"""

df[df.duplicated()] # for checking duplicates

"""## 1.4 Handling Outliers"""

numerical_data=df.select_dtypes(include='number')
numerical_columns=df.select_dtypes(include='number').columns
for col in numerical_data:
  print(f"Outliers in the {col}:-")
  sns.boxplot(df[col])
  plt.xlabel(col)
  plt.show()

# treating outliers with iqr method
# Don't add visibility column because it represents class not values
outlier_columns=['wind_direction','visibility','humidity','average_wind_speed__period_','average_pressure__period_']

for col in outlier_columns:
  Q1=df[col].quantile(0.25)
  Q3=df[col].quantile(0.75)
  iqr=Q3-Q1
  lower=Q1-1.5*iqr
  upper=Q3+1.5*iqr
  median=df[col].median()
  df[col]=df[col].apply(lambda x: median if(x<lower or x>upper ) else x )

for col in numerical_data:
  print(f"Outliers in the {col}:-")
  sns.boxplot(df[col])
  plt.xlabel(col)
  plt.show()

"""## Outliers are treated successfully,lets keep some outliers for model leaning

# 1.6 Standardize the dataframe
"""

from sklearn.preprocessing import StandardScaler,MinMaxScaler
sc=MinMaxScaler()
df_scaled=sc.fit_transform(df)
df= pd.DataFrame(df_scaled, columns=df.columns)

df.head()

"""# 2.EDA

# 2.1 Visualizations
"""

for col in numerical_data:
  plt.title(col)
  sns.histplot(df[col],kde=True)
  plt.xlabel(col)
  plt.ylabel('frequency')
  plt.show()

df['visibility'].unique()
# visibility is a bar graph distributed mostly around 0,so we don't Visibility column in model building

# remove visibility column
df.drop(['visibility'],axis=1,inplace=True)

"""## 2.2 Corelation Matrix"""

df.corr()

"""## 2.3 Visualizing Corelation matrix using HeatMap"""

sns.heatmap(df.corr(),fmt='.3f',annot=True)

# wind speed is positively related to avg-wind-speed and slightly negetively related to avg pressure
# So deleting wind-speed column would be best
# power generated is dependent column , so we need to keep them as we loss much data
df.drop(['wind_speed'],axis=1,inplace=True)

"""# 3.Dataset Division using train-test-split"""

df.head()

from sklearn.model_selection import train_test_split
X=df.iloc[:,0:7]
y=df.iloc[:,7]
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=123)
print(X_train.shape)
print(X_test.shape)

"""# 4.Random Forest Model Building(Default Parameters)"""

print(X_train.columns)

from sklearn.ensemble import RandomForestRegressor
model=RandomForestRegressor(n_estimators=100,random_state=42)
model.fit(X_train,y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2) in RANDOM FOREST MODEL is: {r2:.2f}")

"""# 5.XGBoost(Extrreme Gradient Boosting)"""

!pip install xgboost

import xgboost as xgb
xgb_model=xgb.XGBRegressor(n_estimators=100,random_state=42)
xgb_model.fit(X_train,y_train)

Xgb_model_ypred=xgb_model.predict(X_test)

mae=mean_absolute_error(y_test,Xgb_model_ypred)
mse=mean_squared_error(y_test,Xgb_model_ypred)
rmse=np.sqrt(mse)
r2=r2_score(y_test,Xgb_model_ypred)

print(f"Mean Absolute error (MAE):{mae:.2f}")
print(f"Mean Squared Error(MSE) :{mse:.2f}")
print(f"Root Mean Squared Error(RMSE) :{rmse:.2f}")
print(f"R-squared (R2) in XGBOOST MODEL is: {r2:.2f}")

"""# 6.Light Gradient Boosting Machine"""

!pip install lightgbm

lgbm_model=lgb.LGBMRegressor(n_estimators=100,random_state=25)
lgbm_model.fit(X_train,y_train)

lgbm_model_ypred=lgbm_model.predict(X_test)
mae=mean_absolute_error(y_test,lgbm_model_ypred)
mse=mean_squared_error(y_test,lgbm_model_ypred)
rmse=np.sqrt(mse)
r2=r2_score(y_test,lgbm_model_ypred)

print(f"Mean Absolute Error(MAE):{mae:.2f}")
print(f"Mean Squared Error(MSE):{mse:.2f}")
print(f"Root Mean Squared Error(RMSE):{rmse:.2f}")
print(f"R-squared (R2) in LIGHTGBM MODEL is: {r2:.2f}")

"""# 7. Hyperparameter Tuning with GridSearchCV"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
grid_search_rf = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                              param_grid=param_grid,
                              cv=3,  # 3-fold cross-validation
                              scoring='r2', # Optimize for R-squared
                              n_jobs=-1) # Use all available cores

# Fit the GridSearchCV to the training data
grid_search_rf.fit(X_train, y_train)

# Get the best parameters and the best R-squared score
best_params_rf = grid_search_rf.best_params_
best_r2_rf = grid_search_rf.best_score_

print("Best parameters for Random Forest:", best_params_rf)
print("Best R-squared score (cross-validated) for Random Forest:", best_r2_rf)

# Evaluate the best model on the test data
best_rf_model = grid_search_rf.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)
r2_test_best_rf = r2_score(y_test, y_pred_best_rf)

print(f"R-squared score on the test set with best Random Forest model:-{r2_test_best_rf:.3f}")

