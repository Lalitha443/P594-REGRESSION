# -*- coding: utf-8 -*-
"""P594 REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DB9z4X0DFdKi7YtakFmoEPOu3NOxFyHL
"""

import pandas as pd
import numpy as np
df = pd.read_csv("solarpowergeneration.csv")
df

df.isnull().sum()

df['average-wind-speed-(period)'] = df['average-wind-speed-(period)'].fillna(df['average-wind-speed-(period)'].mean())

"""# Task
Perform a full EDA on the dataframe.

## Understand the data

### Subtask:
Display the first few rows of the dataframe and check the data types and summary statistics.

**Reasoning**:
The first instruction of the subtask is to display the first few rows of the dataframe, the `.head()` method is suitable for this purpose.
"""

df.head()

"""**Reasoning**:
The next instruction is to print the column names and their data types, and then display descriptive statistics. Both can be done with `.info()` and `.describe()` methods respectively.


"""

df.info()
df.describe()

"""## Handle missing values

### Subtask:
Check for missing values and decide on a strategy to handle them (e.g., imputation or removal).

**Reasoning**:
Check for missing values in the dataframe.
"""

df.isnull().sum()

"""## Visualize distributions

### Subtask:
Create visualizations to understand the distribution of each feature (e.g., histograms, box plots).

**Reasoning**:
Create histograms and box plots for each column to visualize their distributions and identify potential outliers.
"""

import matplotlib.pyplot as plt
import seaborn as sns

for column in df.columns:
    plt.figure(figsize=(10, 4))

    plt.subplot(1, 2, 1)
    sns.histplot(df[column], kde=True)
    plt.title(f'Histogram of {column}')

    plt.subplot(1, 2, 2)
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')

    plt.tight_layout()
    plt.show()

"""## Analyze relationships

### Subtask:
Explore the relationships between features and the target variable (`power-generated`) using scatter plots, correlation matrix, etc.

**Reasoning**:
Calculate and display the correlation matrix and then create scatter plots for each feature against the target variable 'power-generated'.
"""

correlation_matrix = df.corr()
display(correlation_matrix)

features = df.columns.drop('power-generated')
for feature in features:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=df[feature], y=df['power-generated'])
    plt.title(f'Scatter plot of {feature} vs Power Generated')
    plt.xlabel(feature)
    plt.ylabel('Power Generated')
    plt.show()

"""## Identify outliers

### Subtask:
Detect and analyze outliers in the data.

**Reasoning**:
Calculate and print the number of outliers for each numerical column using the IQR method.
"""

for column in df.select_dtypes(include=np.number).columns:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"Column '{column}': {len(outliers)} outliers found.")

"""## Summarize findings

### Subtask:
Summarize the key findings from the EDA.

## Summary:

### Data Analysis Key Findings

*   The dataset contains 2919 entries with no missing values.
*   All columns are of numerical data types (`float64` or `int64`).
*   'distance-to-solar-noon' and 'humidity' show strong negative correlations with 'power-generated' (-0.747 and -0.522, respectively).
*   'average-wind-speed-(period)' has a weak positive correlation (0.278) with 'power-generated'.
*   Outliers were detected in several columns, most notably in 'wind-direction' (527), 'visibility' (428), and 'power-generated' (107).

### Insights or Next Steps

*   Investigate the nature and potential impact of the significant number of outliers in 'wind-direction', 'visibility', and 'power-generated'.
*   Consider feature engineering or transformation based on the observed distributions and the strong negative correlations of 'distance-to-solar-noon' and 'humidity' with 'power-generated'.
"""

for column in df.select_dtypes(include=np.number).columns:
    if column not in ['distance-to-solar-noon', 'temperature']:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

print("Shape of the dataframe after removing outliers:", df.shape)

from sklearn.model_selection import train_test_split

X = df.drop('power-generated', axis=1)
y = df['power-generated']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

import matplotlib.pyplot as plt

lgb.plot_importance(lgbm, importance_type='split', figsize=(10, 6))
plt.title('LightGBM Feature Importance (Split)')
plt.show()

lgb.plot_importance(lgbm, importance_type='gain', figsize=(10, 6))
plt.title('LightGBM Feature Importance (Gain)')
plt.show()

import lightgbm as lgb

lgbm = lgb.LGBMRegressor(random_state=42)
lgbm.fit(X_train, y_train)

y_pred_lgbm = lgbm.predict(X_test)

mae_lgbm = mean_absolute_error(y_test, y_pred_lgbm)
mse_lgbm = mean_squared_error(y_test, y_pred_lgbm)
rmse_lgbm = np.sqrt(mse_lgbm)
r2_lgbm = r2_score(y_test, y_pred_lgbm)

print(f"LightGBM Mean Absolute Error (MAE): {mae_lgbm:.2f}")
print(f"LightGBM Mean Squared Error (MSE): {mse_lgbm:.2f}")
print(f"LightGBM Root Mean Squared Error (RMSE): {rmse_lgbm:.2f}")
print(f"LightGBM R-squared (R2): {r2_lgbm:.2f}")

# Re-split the data with the new features
X = df.drop('power-generated', axis=1)
y = df['power-generated']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Retrain the LightGBM model with the new features
lgbm_new_features = lgb.LGBMRegressor(random_state=42)
lgbm_new_features.fit(X_train, y_train)

y_pred_lgbm_new_features = lgbm_new_features.predict(X_test)

mae_lgbm_new_features = mean_absolute_error(y_test, y_pred_lgbm_new_features)
mse_lgbm_new_features = mean_squared_error(y_test, y_pred_lgbm_new_features)
rmse_lgbm_new_features = np.sqrt(mse_lgbm_new_features)
r2_lgbm_new_features = r2_score(y_test, y_pred_lgbm_new_features)

print(f"LightGBM with New Features Mean Absolute Error (MAE): {mae_lgbm_new_features:.2f}")
print(f"LightGBM with New Features Mean Squared Error (MSE): {mse_lgbm_new_features:.2f}")
print(f"LightGBM with New Features Root Mean Squared Error (RMSE): {rmse_lgbm_new_features:.2f}")
print(f"LightGBM with New Features R-squared (R2): {r2_lgbm_new_features:.2f}")

# Commented out IPython magic to ensure Python compatibility.
# %pip install catboost

from catboost import CatBoostRegressor

catboost = CatBoostRegressor(random_state=42, verbose=0) # verbose=0 to reduce output
catboost.fit(X_train, y_train)

y_pred_catboost = catboost.predict(X_test)

mae_catboost = mean_absolute_error(y_test, y_pred_catboost)
mse_catboost = mean_squared_error(y_test, y_pred_catboost)
rmse_catboost = np.sqrt(mse_catboost)
r2_catboost = r2_score(y_test, y_pred_catboost)

print(f"CatBoost Mean Absolute Error (MAE): {mae_catboost:.2f}")
print(f"CatBoost Mean Squared Error (MSE): {mse_catboost:.2f}")
print(f"CatBoost Root Mean Squared Error (RMSE): {rmse_catboost:.2f}")
print(f"CatBoost R-squared (R2): {r2_catboost:.2f}")

# Create interaction features
df['temp_wind_interaction'] = df['temperature'] * df['wind-speed']
df['humidity_sky_interaction'] = df['humidity'] * df['sky-cover']

# Create polynomial features for a few key variables (e.g., distance-to-solar-noon)
df['distance-to-solar-noon_sq'] = df['distance-to-solar-noon']**2

# Display the first few rows with the new features
display(df.head())



"""# Task
Implement K-Fold cross-validation and early stopping for the LightGBM and CatBoost models to evaluate their performance.

## Import necessary libraries

### Subtask:
Import `KFold` from `sklearn.model_selection` and relevant modules for early stopping from `lightgbm` and `catboost`.

**Reasoning**:
Since directly importing `EarlyStopping` is causing issues, I will attempt to implement early stopping by passing a callback function during the CatBoost model training, which is a common pattern in machine learning libraries.
"""

from sklearn.model_selection import KFold
from lightgbm.callback import early_stopping, log_evaluation
from catboost import CatBoostRegressor
# Note: Early stopping for CatBoost is often handled by passing parameters directly
# during the fit method or by using a Pool object with validation data.

# Let's redefine X and y to ensure we are using the latest dataframe with new features
X = df.drop('power-generated', axis=1)
y = df['power-generated']

kf = KFold(n_splits=5, shuffle=True, random_state=42)

catboost_mae_scores = []
catboost_rmse_scores = []
catboost_r2_scores = []

for fold, (train_index, val_index) in enumerate(kf.split(X, y)):
    print(f"Fold {fold+1}")
    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]
    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]

    model_cb = CatBoostRegressor(random_state=42, verbose=0,
                                 early_stopping_rounds=50) # Use early_stopping_rounds

    model_cb.fit(X_train_fold, y_train_fold,
                 eval_set=[(X_val_fold, y_val_fold)],
                 verbose=0) # Pass evaluation set for early stopping

    y_pred_fold = model_cb.predict(X_test)

    mae_fold = mean_absolute_error(y_test, y_pred_fold)
    rmse_fold = np.sqrt(mean_squared_error(y_test, y_pred_fold))
    r2_fold = r2_score(y_test, y_pred_fold)

    catboost_mae_scores.append(mae_fold)
    catboost_rmse_scores.append(rmse_fold)
    catboost_r2_scores.append(r2_fold)

print(f"\nAverage CatBoost MAE across folds: {np.mean(catboost_mae_scores):.2f}")
print(f"Average CatBoost RMSE across folds: {np.mean(catboost_rmse_scores):.2f}")
print(f"Average CatBoost R2 across folds: {np.mean(catboost_r2_scores):.2f}")

from sklearn.model_selection import KFold
from lightgbm.callback import early_stopping, log_evaluation

kf = KFold(n_splits=5, shuffle=True, random_state=42)

lgbm_mae_scores = []
lgbm_rmse_scores = []
lgbm_r2_scores = []

for fold, (train_index, val_index) in enumerate(kf.split(X, y)):
    print(f"Fold {fold+1}")
    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]
    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]

    model_lgb = lgb.LGBMRegressor(random_state=42)

    model_lgb.fit(X_train_fold, y_train_fold,
                  eval_set=[(X_val_fold, y_val_fold)],
                  callbacks=[early_stopping(stopping_rounds=50, verbose=False)]) # Use early_stopping callback

    y_pred_fold = model_lgb.predict(X_test)

    mae_fold = mean_absolute_error(y_test, y_pred_fold)
    rmse_fold = np.sqrt(mean_squared_error(y_test, y_pred_fold))
    r2_fold = r2_score(y_test, y_pred_fold)

    lgbm_mae_scores.append(mae_fold)
    lgbm_rmse_scores.append(rmse_fold)
    lgbm_r2_scores.append(r2_fold)

print(f"\nAverage LightGBM MAE across folds: {np.mean(lgbm_mae_scores):.2f}")
print(f"Average LightGBM RMSE across folds: {np.mean(lgbm_rmse_scores):.2f}")
print(f"Average LightGBM R2 across folds: {np.mean(lgbm_r2_scores):.2f}")

y_pred_final = model_lgb.predict(X_test)
print("Predictions on the test set:")
display(y_pred_final)



"""# Task
Deploy the trained LightGBM model to predict energy production.

## Save the trained model

### Subtask:
Save the trained LightGBM model to a file using a library like `joblib` or `pickle`.

**Reasoning**:
Save the trained LightGBM model to a file using joblib.
"""

import joblib

joblib.dump(model_lgb, 'lightgbm_model.joblib')

"""## Create a prediction script/api

### Subtask:
Develop a script or API endpoint that loads the saved model and takes new data as input to make predictions.

**Reasoning**:
Import necessary libraries and define a function to load the model and make predictions.
"""

import joblib
import pandas as pd

def predict_power_generation(new_data):
    """
    Loads the saved LightGBM model and makes predictions on new data.

    Args:
        new_data (pd.DataFrame): DataFrame containing the new data with the same
                                   features as the training data.

    Returns:
        np.ndarray: Array of predicted power generation values.
    """
    # Load the trained model
    loaded_model = joblib.load('lightgbm_model.joblib')

    # Make predictions on the new data
    predictions = loaded_model.predict(new_data)

    return predictions

# Example usage (replace with your actual new data)
# Assuming 'new_data_df' is a pandas DataFrame with the same columns as X
# new_data_df = pd.DataFrame(...)
# predictions = predict_power_generation(new_data_df)
# print(predictions)

"""## Test the deployed model

### Subtask:
Test the deployed model to ensure it is working correctly and providing accurate predictions.

**Reasoning**:
Create a sample DataFrame with the same columns and data types as the training data, and then use the defined prediction function to make predictions and print the results.
"""

# 1. Create a small sample DataFrame
new_data_df = pd.DataFrame({
    'distance-to-solar-noon': [0.5, 0.2, 0.8],
    'temperature': [70, 65, 75],
    'wind-direction': [25, 30, 20],
    'wind-speed': [8.0, 5.5, 10.0],
    'sky-cover': [1, 0, 2],
    'visibility': [10.0, 9.0, 10.0],
    'humidity': [70, 80, 60],
    'average-wind-speed-(period)': [7.5, 6.0, 11.0],
    'average-pressure-(period)': [29.9, 30.0, 29.8],
    'temp_wind_interaction': [70 * 8.0, 65 * 5.5, 75 * 10.0],
    'humidity_sky_interaction': [70 * 1, 80 * 0, 60 * 2],
    'distance-to-solar-noon_sq': [0.5**2, 0.2**2, 0.8**2]
})

# Ensure the order of columns is the same as X_train
new_data_df = new_data_df[X_train.columns]


# 2. Use the predict_power_generation function to make predictions
predictions = predict_power_generation(new_data_df)

# 3. Print the resulting predictions
print("Predictions for the new data:")
print(predictions)

"""## Summary:

### Data Analysis Key Findings

*   The trained LightGBM model was successfully saved to a file named `lightgbm_model.joblib` using `joblib.dump()`.
*   A Python function `predict_power_generation` was created to load the saved model and make predictions on new data.
*   The `predict_power_generation` function was successfully tested with a sample DataFrame, demonstrating its ability to load the model and generate predictions.

### Insights or Next Steps

*   The created `predict_power_generation` function can be integrated into an API endpoint for real-time predictions.
*   Further testing with a larger and more diverse dataset is recommended to fully validate the model's performance in a production environment.

# Task
Deploy the trained LightGBM model using Streamlit to predict energy production based on environmental variables.

## Save the trained model

### Subtask:
Save the trained LightGBM model to a file using a library like `joblib` or `pickle`.

## Install streamlit

### Subtask:
Install the Streamlit library.

**Reasoning**:
Install the streamlit library using pip.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

"""## Create a streamlit application script

### Subtask:
Write a Python script that will serve as your Streamlit application.

**Reasoning**:
Create the Streamlit script with necessary imports and title.
"""

import streamlit as st
import joblib
import pandas as pd

st.title('Solar Power Generation Prediction')

"""**Reasoning**:
Define a function to load the model and add input fields for the features in the Streamlit app.


"""

def load_model():
    """Loads the trained LightGBM model."""
    return joblib.load('lightgbm_model.joblib')

model = load_model()

st.header('Enter Environmental Variables')

# Create input fields for each feature
distance_to_solar_noon = st.number_input('Distance to Solar Noon', min_value=0.0, max_value=1.2, value=0.5)
temperature = st.number_input('Temperature', min_value=0, max_value=100, value=70)
wind_direction = st.number_input('Wind Direction', min_value=0, max_value=360, value=25)
wind_speed = st.number_input('Wind Speed', min_value=0.0, max_value=30.0, value=8.0)
sky_cover = st.number_input('Sky Cover', min_value=0, max_value=4, value=1)
visibility = st.number_input('Visibility', min_value=0.0, max_value=10.0, value=10.0)
humidity = st.number_input('Humidity', min_value=0, max_value=100, value=70)
average_wind_speed_period = st.number_input('Average Wind Speed (Period)', min_value=0.0, max_value=50.0, value=7.5)
average_pressure_period = st.number_input('Average Pressure (Period)', min_value=28.0, max_value=32.0, value=29.9)

# Feature engineering for interaction and polynomial terms
temp_wind_interaction = temperature * wind_speed
humidity_sky_interaction = humidity * sky_cover
distance_to_solar_noon_sq = distance_to_solar_noon**2

"""**Reasoning**:
Add a button to trigger predictions and display the prediction result.


"""

if st.button('Predict Power Generation'):
    # Create a DataFrame from user inputs, ensuring column order matches training data
    input_data = pd.DataFrame({
        'distance-to-solar-noon': [distance_to_solar_noon],
        'temperature': [temperature],
        'wind-direction': [wind_direction],
        'wind-speed': [wind_speed],
        'sky-cover': [sky_cover],
        'visibility': [visibility],
        'humidity': [humidity],
        'average-wind-speed-(period)': [average_wind_speed_period],
        'average-pressure-(period)': [average_pressure_period],
        'temp_wind_interaction': [temp_wind_interaction],
        'humidity_sky_interaction': [humidity_sky_interaction],
        'distance-to-solar-noon_sq': [distance_to_solar_noon_sq]
    })

    # Ensure the column order is the same as the training data (using X_train from the notebook)
    # This assumes X_train is available in the environment where the script is run
    # In a real deployment, you would save and load the column order or ensure consistency
    # For this example, we'll assume the order created matches the training data for simplicity
    # A more robust solution would involve saving the column order during training

    prediction = model.predict(input_data)
    st.subheader('Predicted Power Generation')
    st.write(f'{prediction[0]:,.2f} kWh')

"""## Load the model in the streamlit app

### Subtask:
In the Streamlit script, load the saved LightGBM model.

**Reasoning**:
Define the Streamlit application script to load the model and take user inputs.
"""

import streamlit as st
import joblib
import pandas as pd
import numpy as np

st.title('Solar Power Generation Prediction')

# Define a function to load the model
@st.cache_resource
def load_model():
    """Loads the saved LightGBM model."""
    return joblib.load('lightgbm_model.joblib')

# Load the model
model = load_model()

st.header('Enter Environmental Variables:')

# Create input fields for the features
distance_to_solar_noon = st.number_input('Distance to Solar Noon', min_value=0.0, max_value=1.5, value=0.5, step=0.01)
temperature = st.number_input('Temperature (°F)', min_value=0, max_value=100, value=70)
wind_direction = st.number_input('Wind Direction (°)', min_value=0, max_value=360, value=25)
wind_speed = st.number_input('Wind Speed (mph)', min_value=0.0, max_value=50.0, value=8.0, step=0.1)
sky_cover = st.number_input('Sky Cover (0-4)', min_value=0, max_value=4, value=1)
visibility = st.number_input('Visibility (miles)', min_value=0.0, max_value=10.0, value=10.0, step=0.1)
humidity = st.number_input('Humidity (%)', min_value=0, max_value=100, value=70)
average_wind_speed_period = st.number_input('Average Wind Speed (Period)', min_value=0.0, max_value=50.0, value=7.5, step=0.1)
average_pressure_period = st.number_input('Average Pressure (Period)', min_value=28.0, max_value=32.0, value=29.9, step=0.01)

# Feature engineering based on the training data
temp_wind_interaction = temperature * wind_speed
humidity_sky_interaction = humidity * sky_cover
distance_to_solar_noon_sq = distance_to_solar_noon**2

if st.button('Predict Power Generation'):
    # Create a DataFrame from user inputs, ensuring column order matches training data
    input_data = pd.DataFrame({
        'distance-to-solar-noon': [distance_to_solar_noon],
        'temperature': [temperature],
        'wind-direction': [wind_direction],
        'wind-speed': [wind_speed],
        'sky-cover': [sky_cover],
        'visibility': [visibility],
        'humidity': [humidity],
        'average-wind-speed-(period)': [average_wind_speed_period],
        'average-pressure-(period)': [average_pressure_period],
        'temp_wind_interaction': [temp_wind_interaction],
        'humidity_sky_interaction': [humidity_sky_interaction],
        'distance-to-solar-noon_sq': [distance_to_solar_noon_sq]
    })

    # Ensure the column order is the same as the training data (using X_train from the notebook)
    # This assumes X_train is available in the environment where the script is run
    # In a real deployment, you would save and load the column order or ensure consistency
    # For this example, we'll assume the order created matches the training data for simplicity
    # A more robust solution would involve saving the column order during training

    prediction = model.predict(input_data)
    st.subheader('Predicted Power Generation')
    st.write(f'{prediction[0]:,.2f} kWh')

"""## Create a user interface

### Subtask:
Design a user interface in the Streamlit script to allow users to input environmental variables.

**Reasoning**:
Add a header and input widgets for each environmental variable using st.header() and st.number_input() with appropriate labels, ranges, and default values.
"""

import streamlit as st
import joblib
import pandas as pd
import numpy as np

st.title('Solar Power Generation Prediction')

# Define a function to load the model
@st.cache_resource
def load_model():
    """Loads the saved LightGBM model."""
    return joblib.load('lightgbm_model.joblib')

# Load the model
model = load_model()

st.header('Enter Environmental Variables:')

# Create input fields for the features
distance_to_solar_noon = st.number_input('Distance to Solar Noon', min_value=0.0, max_value=1.5, value=0.5, step=0.01)
temperature = st.number_input('Temperature (°F)', min_value=0, max_value=100, value=70)
wind_direction = st.number_input('Wind Direction (°)', min_value=0, max_value=360, value=25)
wind_speed = st.number_input('Wind Speed (mph)', min_value=0.0, max_value=50.0, value=8.0, step=0.1)
sky_cover = st.number_input('Sky Cover (0-4)', min_value=0, max_value=4, value=1)
visibility = st.number_input('Visibility (miles)', min_value=0.0, max_value=10.0, value=10.0, step=0.1)
humidity = st.number_input('Humidity (%)', min_value=0, max_value=100, value=70)
average_wind_speed_period = st.number_input('Average Wind Speed (Period)', min_value=0.0, max_value=50.0, value=7.5, step=0.1)
average_pressure_period = st.number_input('Average Pressure (Period)', min_value=28.0, max_value=32.0, value=29.9, step=0.01)

# Feature engineering based on the training data
temp_wind_interaction = temperature * wind_speed
humidity_sky_interaction = humidity * sky_cover
distance_to_solar_noon_sq = distance_to_solar_noon**2

if st.button('Predict Power Generation'):
    # Create a DataFrame from user inputs, ensuring column order matches training data
    input_data = pd.DataFrame({
        'distance-to-solar-noon': [distance_to_solar_noon],
        'temperature': [temperature],
        'wind-direction': [wind_direction],
        'wind-speed': [wind_speed],
        'sky-cover': [sky_cover],
        'visibility': [visibility],
        'humidity': [humidity],
        'average-wind-speed-(period)': [average_wind_speed_period],
        'average-pressure-(period)': [average_pressure_period],
        'temp_wind_interaction': [temp_wind_interaction],
        'humidity_sky_interaction': [humidity_sky_interaction],
        'distance-to-solar-noon_sq': [distance_to_solar_noon_sq]
    })

    # Ensure the column order is the same as the training data (using X_train from the notebook)
    # This assumes X_train is available in the environment where the script is run
    # In a real deployment, you would save and load the column order or ensure consistency
    # For this example, we'll assume the order created matches the training data for simplicity
    # A more robust solution would involve saving the column order during training

    prediction = model.predict(input_data)
    st.subheader('Predicted Power Generation')
    st.write(f'{prediction[0]:,.2f} kWh')

"""## Make predictions in the streamlit app

### Subtask:
Use the loaded model to make predictions based on the user's input.

**Reasoning**:
Use the loaded model to make predictions based on the user's input.
"""

import streamlit as st
import joblib
import pandas as pd
import numpy as np

st.title('Solar Power Generation Prediction')

# Define a function to load the model
@st.cache_resource
def load_model():
    """Loads the saved LightGBM model."""
    return joblib.load('lightgbm_model.joblib')

# Load the model
model = load_model()

st.header('Enter Environmental Variables:')

# Create input fields for the features
distance_to_solar_noon = st.number_input('Distance to Solar Noon', min_value=0.0, max_value=1.5, value=0.5, step=0.01)
temperature = st.number_input('Temperature (°F)', min_value=0, max_value=100, value=70)
wind_direction = st.number_input('Wind Direction (°)', min_value=0, max_value=360, value=25)
wind_speed = st.number_input('Wind Speed (mph)', min_value=0.0, max_value=50.0, value=8.0, step=0.1)
sky_cover = st.number_input('Sky Cover (0-4)', min_value=0, max_value=4, value=1)
visibility = st.number_input('Visibility (miles)', min_value=0.0, max_value=10.0, value=10.0, step=0.1)
humidity = st.number_input('Humidity (%)', min_value=0, max_value=100, value=70)
average_wind_speed_period = st.number_input('Average Wind Speed (Period)', min_value=0.0, max_value=50.0, value=7.5, step=0.1)
average_pressure_period = st.number_input('Average Pressure (Period)', min_value=28.0, max_value=32.0, value=29.9, step=0.01)

# Feature engineering based on the training data
temp_wind_interaction = temperature * wind_speed
humidity_sky_interaction = humidity * sky_cover
distance_to_solar_noon_sq = distance_to_solar_noon**2

if st.button('Predict Power Generation'):
    # Create a DataFrame from user inputs, ensuring column order matches training data
    input_data = pd.DataFrame({
        'distance-to-solar-noon': [distance_to_solar_noon],
        'temperature': [temperature],
        'wind-direction': [wind_direction],
        'wind-speed': [wind_speed],
        'sky-cover': [sky_cover],
        'visibility': [visibility],
        'humidity': [humidity],
        'average-wind-speed-(period)': [average_wind_speed_period],
        'average-pressure-(period)': [average_pressure_period],
        'temp_wind_interaction': [temp_wind_interaction],
        'humidity_sky_interaction': [humidity_sky_interaction],
        'distance-to-solar-noon_sq': [distance_to_solar_noon_sq]
    })

    # Ensure the column order is the same as the training data (using X_train from the notebook)
    # This assumes X_train is available in the environment where the script is run
    # In a real deployment, you would save and load the column order or ensure consistency
    # For this example, we'll assume the order created matches the training data for simplicity
    # A more robust solution would involve saving the column order during training
    if 'X_train' in locals():
        input_data = input_data[X_train.columns]
    else:
        # Fallback in case X_train is not available (e.g., in a separate script)
        # Define the expected column order manually or load it from a saved list
        expected_columns = ['distance-to-solar-noon', 'temperature', 'wind-direction', 'wind-speed', 'sky-cover', 'visibility', 'humidity', 'average-wind-speed-(period)', 'average-pressure-(period)', 'temp_wind_interaction', 'humidity_sky_interaction', 'distance-to-solar-noon_sq']
        input_data = input_data[expected_columns]


    prediction = model.predict(input_data)
    st.subheader('Predicted Power Generation')
    st.write(f'{prediction[0]:,.2f} kWh')

"""## Display the predictions

### Subtask:
Show the predicted energy production to the user in the Streamlit app.

**Reasoning**:
Add the necessary Streamlit display components to the script to show the predicted energy production to the user, formatted as specified.
"""

import streamlit as st
import joblib
import pandas as pd
import numpy as np

st.title('Solar Power Generation Prediction')

# Define a function to load the model
@st.cache_resource
def load_model():
    """Loads the saved LightGBM model."""
    return joblib.load('lightgbm_model.joblib')

# Load the model
model = load_model()

st.header('Enter Environmental Variables:')

# Create input fields for the features
distance_to_solar_noon = st.number_input('Distance to Solar Noon', min_value=0.0, max_value=1.5, value=0.5, step=0.01)
temperature = st.number_input('Temperature (°F)', min_value=0, max_value=100, value=70)
wind_direction = st.number_input('Wind Direction (°)', min_value=0, max_value=360, value=25)
wind_speed = st.number_input('Wind Speed (mph)', min_value=0.0, max_value=50.0, value=8.0, step=0.1)
sky_cover = st.number_input('Sky Cover (0-4)', min_value=0, max_value=4, value=1)
visibility = st.number_input('Visibility (miles)', min_value=0.0, max_value=10.0, value=10.0, step=0.1)
humidity = st.number_input('Humidity (%)', min_value=0, max_value=100, value=70)
average_wind_speed_period = st.number_input('Average Wind Speed (Period)', min_value=0.0, max_value=50.0, value=7.5, step=0.1)
average_pressure_period = st.number_input('Average Pressure (Period)', min_value=28.0, max_value=32.0, value=29.9, step=0.01)

# Feature engineering based on the training data
temp_wind_interaction = temperature * wind_speed
humidity_sky_interaction = humidity * sky_cover
distance_to_solar_noon_sq = distance_to_solar_noon**2

if st.button('Predict Power Generation'):
    # Create a DataFrame from user inputs, ensuring column order matches training data
    input_data = pd.DataFrame({
        'distance-to-solar-noon': [distance_to_solar_noon],
        'temperature': [temperature],
        'wind-direction': [wind_direction],
        'wind-speed': [wind_speed],
        'sky-cover': [sky_cover],
        'visibility': [visibility],
        'humidity': [humidity],
        'average-wind-speed-(period)': [average_wind_speed_period],
        'average-pressure-(period)': [average_pressure_period],
        'temp_wind_interaction': [temp_wind_interaction],
        'humidity_sky_interaction': [humidity_sky_interaction],
        'distance-to-solar-noon_sq': [distance_to_solar_noon_sq]
    })

    # Ensure the column order is the same as the training data (using X_train from the notebook)
    # This assumes X_train is available in the environment where the script is run
    # In a real deployment, you would save and load the column order or ensure consistency
    # For this example, we'll assume the order created matches the training data for simplicity
    # A more robust solution would involve saving the column order during training
    if 'X_train' in locals():
        input_data = input_data[X_train.columns]
    else:
        # Fallback in case X_train is not available (e.g., in a separate script)
        # Define the expected column order manually or load it from a saved list
        expected_columns = ['distance-to-solar-noon', 'temperature', 'wind-direction', 'wind-speed', 'sky-cover', 'visibility', 'humidity', 'average-wind-speed-(period)', 'average-pressure-(period)', 'temp_wind_interaction', 'humidity_sky_interaction', 'distance-to-solar-noon_sq']
        input_data = input_data[expected_columns]


    prediction = model.predict(input_data)
    st.subheader('Predicted Power Generation')
    st.write(f'{prediction[0]:,.2f} kWh')

"""## Run the streamlit application

### Subtask:
Execute the Streamlit script to run the web application locally.

## Deploy the streamlit application (optional)

### Subtask:
Deploy the Streamlit application to a hosting platform (e.g., Streamlit Cloud, Heroku, etc.).

## Summary:

### Data Analysis Key Findings

*   The trained LightGBM model was successfully saved to a file named `lightgbm_model.joblib` using `joblib.dump()`.
*   The Streamlit library (version 1.50.0) and its dependencies were successfully installed.
*   A Python script for the Streamlit application was created, including necessary imports, a function to load the model, a title, input fields for environmental variables using `st.number_input`, feature engineering steps, a prediction button, and logic to display the prediction.
*   The script successfully loads the saved LightGBM model using `joblib.load()` and utilizes `@st.cache_resource` for caching.
*   The Streamlit script includes a header for the input section and appropriate input widgets for all required environmental variables.
*   The script successfully integrates Streamlit's interactive components with the loaded model, processes user input into a DataFrame, applies feature engineering, uses the model's `predict()` method, and displays the prediction with a subheader and formatted output.
*   The tasks of running and deploying the Streamlit application could not be completed in the current environment, as they require a terminal and manual deployment steps, respectively.

### Insights or Next Steps

*   The Streamlit script is ready to be executed in a suitable environment to launch the interactive web application for power generation prediction.
*   To fully deploy the application, manual steps are required to set up hosting on a platform like Streamlit Cloud or Heroku and configure the deployment process.
"""